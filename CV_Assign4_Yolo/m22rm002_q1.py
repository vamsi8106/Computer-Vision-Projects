# -*- coding: utf-8 -*-
"""M22RM002_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XHq-jwwqasn8OD8Q3ObKeD3SM1YHlir2
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
# Normalize pixel values between 0 and 1
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

# Convert labels to one-hot encoded format
y_train = keras.utils.to_categorical(y_train, num_classes=10)
y_test = keras.utils.to_categorical(y_test, num_classes=10)

model = keras.Sequential(
    [
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu", input_shape=(28, 28, 1)),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax"),
    ]
)
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(x_train.reshape(-1, 28, 28, 1), y_train, batch_size=128, epochs=10, validation_split=0.1)
test_loss, test_acc = model.evaluate(x_test.reshape(-1, 28, 28, 1), y_test)
print("Test accuracy:", test_acc)

import numpy as np
import matplotlib.pyplot as plt

test_index = np.random.randint(0, x_test.shape[0])
test_image = x_test[test_index]
test_label = y_test[test_index]

prediction = model.predict(test_image.reshape(-1, 28, 28, 1)).argmax()
plt.imshow(test_image.squeeze(), cmap="gray")
plt.title(f"True label: {np.argmax(test_label)}, Predicted label: {prediction}")
plt.show()

"""# Part-3
Report the following on test data: (should be implemented from scratch) 
a. Confusion matrix


"""

# Make predictions on the test set
y_pred = model.predict(x_test.reshape(-1, 28, 28, 1))

# Convert predictions from one-hot encoding to labels
y_pred_labels = np.argmax(y_pred, axis=1)
y_test_labels = np.argmax(y_test, axis=1)

def confusion_matrix(y_true, y_pred, num_classes):
    # Initialize confusion matrix with zeros
    matrix = np.zeros((num_classes, num_classes))

    # Iterate through each prediction
    for i in range(len(y_true)):
        # Increment the count of true positive
        matrix[y_true[i]][y_pred[i]] += 1

    return matrix

import numpy as np


# Compute confusion matrix
num_classes = 10
conf_matrix = confusion_matrix(y_test_labels, y_pred_labels, num_classes)

# Print confusion matrix
print("Confusion Matrix:")
print(np.array2string(conf_matrix, separator='  ', 
                       formatter={'float_kind':lambda x: "%.0f" % x}))

"""b. Overall and classwise accuracy.

"""

from sklearn.metrics import classification_report

# Compute classification report
class_report = classification_report(y_test_labels, y_pred_labels)
print("Classification Report:")
print(class_report)

# Compute overall accuracy
overall_acc = np.sum(y_pred_labels == y_test_labels) / len(y_test_labels)
print("Overall Accuracy: {:.2f}%".format(overall_acc * 100))

# Compute classwise accuracy
classwise_acc = []
for i in range(10):
    class_acc = np.sum((y_pred_labels == i) & (y_test_labels == i)) / np.sum(y_test_labels == i)
    classwise_acc.append(class_acc)
    print("Class {} Accuracy: {:.2f}%".format(i, class_acc * 100))

"""c. ROC curve.

Overall Roc Plot
"""

import numpy as np
import matplotlib.pyplot as plt

# Example ground truth labels and predictions
y_true = y_test
y_scores = y_pred

# Compute ROC curve
thresholds = np.sort(np.unique(y_scores))[::-1]
fpr = []
tpr = []
for threshold in thresholds:
    tp = np.sum((y_scores >= threshold) & (y_true == 1))
    tn = np.sum((y_scores < threshold) & (y_true == 0))
    fp = np.sum((y_scores >= threshold) & (y_true == 0))
    fn = np.sum((y_scores < threshold) & (y_true == 1))
    tpr.append(tp / (tp + fn))
    fpr.append(fp / (fp + tn))
fpr = np.array(fpr)
tpr = np.array(tpr)

# Compute AUC
auc_score = np.trapz(tpr, fpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""Individual Class Roc Plot with inbuilt function"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Example ground truth labels and predictions
y_true = y_test
y_scores = y_pred

# Compute ROC curve and ROC area for each class
#n_classes = len(np.unique(y_true))
n_classes=10
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_true[:, i], y_scores[:, i])
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve for the current class
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic (ROC) curve for class {}'.format(i))
    plt.legend(loc="lower right")

    # Save the figure to a file
    plt.savefig('roc_curve_class{}.png'.format(i))

    # Show the plot
    plt.show()

"""Individual Class Roc Plot from scratch"""

import numpy as np
import matplotlib.pyplot as plt

# Example ground truth labels and predictions
y_true = y_test
y_scores = y_pred

# Compute ROC curve and ROC area for each class
n_classes = y_true.shape[1]
for i in range(n_classes):
    # Find indices of true positive and false positive samples
    tp_indices = np.where((y_true[:, i] == 1) & (y_scores[:, i] >= 0.5))[0]
    fp_indices = np.where((y_true[:, i] == 0) & (y_scores[:, i] >= 0.5))[0]

    # Calculate true positive rate and false positive rate
    tpr = len(tp_indices) / len(np.where(y_true[:, i] == 1)[0])
    fpr = len(fp_indices) / len(np.where(y_true[:, i] == 0)[0])

    # Sort indices by decreasing score
    indices = np.argsort(y_scores[:, i])[::-1]

    # Calculate the thresholds and the ROC points
    thresholds = np.unique(y_scores[:, i])[::-1]
    roc_points = np.zeros((len(thresholds), 2))
    for j, threshold in enumerate(thresholds):
        tp = len(np.intersect1d(np.where(y_scores[indices] >= threshold)[0], tp_indices))
        fp = len(np.intersect1d(np.where(y_scores[indices] >= threshold)[0], fp_indices))
        tn = len(np.intersect1d(np.where(y_scores[indices] < threshold)[0], fp_indices))
        fn = len(np.intersect1d(np.where(y_scores[indices] < threshold)[0], tp_indices))
        tpr = tp / (tp + fn)
        fpr = fp / (fp + tn)
        roc_points[j] = [fpr, tpr]

    # Calculate the AUC
    auc_score = np.trapz(roc_points[:, 1], roc_points[:, 0])

    # Plot the ROC curve
    plt.figure()
    plt.plot(roc_points[:, 0], roc_points[:, 1], color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic (ROC) curve for class {}'.format(i))
    plt.legend(loc="lower right")

    # Save the figure to a file
    plt.savefig('roc_curve_class{}.png'.format(i))

    # Show the plot
    plt.show()

"""# Part-4
 Report loss curve during training.
"""

import numpy as np
import matplotlib.pyplot as plt

# Compile the model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model on the training set
history = model.fit(x_train.reshape(-1, 28, 28, 1), y_train, batch_size=128, epochs=10, validation_split=0.1)

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test.reshape(-1, 28, 28, 1), y_test)

# Make predictions on the test set
y_pred = model.predict(x_test.reshape(-1, 28, 28, 1))

# Plot the loss curve
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

"""# Part-5
Replace your CNN with resnet18 and compare it with all metrics given in part 3
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, MaxPooling2D, AveragePooling2D, Flatten, Dense
from tensorflow.keras.models import Model

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize and reshape the data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape((-1, 28, 28, 1))
x_test = x_test.reshape((-1, 28, 28, 1))
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, MaxPooling2D, AveragePooling2D, Flatten, Dense
from tensorflow.keras.models import Model

def Conv2D_BN(x, filters, kernel_size, strides=(1,1), padding='same'):
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    return x

def Residual_Block(x, filters, strides=(1, 1)):
    shortcut = x
    x = Conv2D_BN(x, filters, kernel_size=(3, 3), strides=strides)
    x = Conv2D_BN(x, filters, kernel_size=(3, 3))
    
    if strides != (1, 1) or x.shape[-1] != shortcut.shape[-1]:
        shortcut = Conv2D_BN(shortcut, filters, kernel_size=(1, 1), strides=strides)

    x = Add()([x, shortcut])
    x = Activation('relu')(x)
    return x

def ResNet18(input_shape, num_classes):
    input = Input(shape=input_shape)
    x = Conv2D_BN(input, 64, kernel_size=(7, 7), strides=(2,2))
    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)

    x = Residual_Block(x, 64)
    x = Residual_Block(x, 64)

    x = Residual_Block(x, 128, strides=(2, 2))
    x = Residual_Block(x, 128)

    x = Residual_Block(x, 256, strides=(2, 2))
    x = Residual_Block(x, 256)

    x = Residual_Block(x, 512, strides=(2, 2))
    x = Residual_Block(x, 512)

    x = AveragePooling2D(pool_size=(1,1))(x)
    x = Flatten()(x)
    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input, outputs=output)
    return model


model = ResNet18(input_shape=(28, 28, 1), num_classes=10)
model.summary()

# Compile and fit the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))

import numpy as np
import matplotlib.pyplot as plt

test_index = np.random.randint(0, x_test.shape[0])
test_image = x_test[test_index]
test_label = y_test[test_index]

prediction = model.predict(test_image.reshape(-1, 28, 28, 1)).argmax()
plt.imshow(test_image.squeeze(), cmap="gray")
plt.title(f"True label: {np.argmax(test_label)}, Predicted label: {prediction}")
plt.show()

"""a. Confusion matrix
b. Overall and classwise accuracy.
"""

# Make predictions on the test set
y_pred = model.predict(x_test.reshape(-1, 28, 28, 1))

# Convert predictions from one-hot encoding to labels
y_pred_labels = np.argmax(y_pred, axis=1)
y_test_labels = np.argmax(y_test, axis=1)

import numpy as np


# Compute confusion matrix
num_classes = 10
conf_matrix = confusion_matrix(y_test_labels, y_pred_labels, num_classes)

# Print confusion matrix
print("Confusion Matrix:")
print(np.array2string(conf_matrix, separator='  ', 
                       formatter={'float_kind':lambda x: "%.0f" % x}))

# Compute overall accuracy
overall_acc = np.sum(y_pred_labels == y_test_labels) / len(y_test_labels)
print("Overall Accuracy: {:.2f}%".format(overall_acc * 100))
# Compute classwise accuracy
classwise_acc = []
for i in range(10):
    class_acc = np.sum((y_pred_labels == i) & (y_test_labels == i)) / np.sum(y_test_labels == i)
    classwise_acc.append(class_acc)
    print("Class {} Accuracy: {:.2f}%".format(i, class_acc * 100))

"""c. ROC curve.

Overall ROC Plot
"""

import numpy as np
import matplotlib.pyplot as plt

# Example ground truth labels and predictions
y_true = y_test
y_scores = y_pred

# Compute ROC curve
thresholds = np.sort(np.unique(y_scores))[::-1]
fpr = []
tpr = []
for threshold in thresholds:
    tp = np.sum((y_scores >= threshold) & (y_true == 1))
    tn = np.sum((y_scores < threshold) & (y_true == 0))
    fp = np.sum((y_scores >= threshold) & (y_true == 0))
    fn = np.sum((y_scores < threshold) & (y_true == 1))
    tpr.append(tp / (tp + fn))
    fpr.append(fp / (fp + tn))
fpr = np.array(fpr)
tpr = np.array(tpr)

# Compute AUC
auc_score = np.trapz(tpr, fpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""Individual Class ROC plot using inbuilt function"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Example ground truth labels and predictions
y_true = y_test
y_scores = y_pred

# Compute ROC curve and ROC area for each class
#n_classes = len(np.unique(y_true))
n_classes=10
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_true[:, i], y_scores[:, i])
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve for the current class
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic (ROC) curve for class {}'.format(i))
    plt.legend(loc="lower right")

    # Save the figure to a file
    plt.savefig('roc_curve_class{}.png'.format(i))

    # Show the plot
    plt.show()

"""ROC plot of Individual class from scratch"""

import numpy as np
import matplotlib.pyplot as plt

# Example ground truth labels and predictions
y_true = y_test
y_scores = y_pred

# Compute ROC curve and ROC area for each class
n_classes = y_true.shape[1]
for i in range(n_classes):
    # Find indices of true positive and false positive samples
    tp_indices = np.where((y_true[:, i] == 1) & (y_scores[:, i] >= 0.5))[0]
    fp_indices = np.where((y_true[:, i] == 0) & (y_scores[:, i] >= 0.5))[0]

    # Calculate true positive rate and false positive rate
    tpr = len(tp_indices) / len(np.where(y_true[:, i] == 1)[0])
    fpr = len(fp_indices) / len(np.where(y_true[:, i] == 0)[0])

    # Sort indices by decreasing score
    indices = np.argsort(y_scores[:, i])[::-1]

    # Calculate the thresholds and the ROC points
    thresholds = np.unique(y_scores[:, i])[::-1]
    roc_points = np.zeros((len(thresholds), 2))
    for j, threshold in enumerate(thresholds):
        tp = len(np.intersect1d(np.where(y_scores[indices] >= threshold)[0], tp_indices))
        fp = len(np.intersect1d(np.where(y_scores[indices] >= threshold)[0], fp_indices))
        tn = len(np.intersect1d(np.where(y_scores[indices] < threshold)[0], fp_indices))
        fn = len(np.intersect1d(np.where(y_scores[indices] < threshold)[0], tp_indices))
        tpr = tp / (tp + fn)
        fpr = fp / (fp + tn)
        roc_points[j] = [fpr, tpr]

    # Calculate the AUC
    auc_score = np.trapz(roc_points[:, 1], roc_points[:, 0])

    # Plot the ROC curve
    plt.figure()
    plt.plot(roc_points[:, 0], roc_points[:, 1], color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic (ROC) curve for class {}'.format(i))
    plt.legend(loc="lower right")

    # Save the figure to a file
    plt.savefig('roc_curve_class{}.png'.format(i))

    # Show the plot
    plt.show()

# Compute classification report
class_report = classification_report(y_test_labels, y_pred_labels)
print("Classification Report:")
print(class_report)

